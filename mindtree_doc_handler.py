import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from mindtree_graph import build_knowledge_graph
from rank_bm25 import BM25Okapi
import os
import re
from logger_config import logger
from tqdm import tqdm


# å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£
# è¯¥å‡½æ•°è§£ææ–‡æ¡£å†…å®¹ï¼Œå°†å…¶åˆ†å‰²ï¼Œå¹¶å­˜å‚¨åœ¨ä¸åŒçš„æ£€ç´¢ç»“æ„ä¸­
def process_documents(uploaded_files, reranker, embedding_model, base_url):
    if st.session_state.documents_loaded:
        logger.info("Documents already loaded, skipping processing")
        return

    st.session_state.processing = True
    documents = []
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    if not os.path.exists("temp"):
        os.makedirs("temp")
        logger.info("Created temporary directory")
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # å¤„ç†æ–‡ä»¶
        for i, file in enumerate(uploaded_files):
            try:
                status_text.text(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i+1}/{len(uploaded_files)}: {file.name}")
                progress_bar.progress((i + 1) / len(uploaded_files))
                
                file_path = os.path.join("temp", file.name)
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())
                logger.info(f"Saved temporary file: {file_path}")

                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
                if file.name.endswith(".pdf"):
                    loader = PyPDFLoader(file_path)
                elif file.name.endswith(".docx"):
                    loader = Docx2txtLoader(file_path)
                elif file.name.endswith(".txt"):
                    loader = TextLoader(file_path)
                else:
                    logger.warning(f"Unsupported file type: {file.name}")
                    continue
                
                # åŠ è½½æ–‡æ¡£
                docs = loader.load()
                documents.extend(docs)
                logger.info(f"Loaded {len(docs)} pages from {file.name}")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(file_path)
                logger.info(f"Removed temporary file: {file_path}")
                
            except Exception as e:
                logger.error(f"Error processing file {file.name}: {str(e)}")
                st.error(f"å¤„ç†æ–‡ä»¶ {file.name} æ—¶å‡ºé”™: {str(e)}")
                continue

        if not documents:
            raise ValueError("No documents were successfully processed")

        # æ–‡æœ¬åˆ†å‰²
        status_text.text("æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
        text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=200, separator="\n")
        texts = text_splitter.split_documents(documents)
        text_contents = [doc.page_content for doc in texts]
        logger.info(f"Split documents into {len(texts)} chunks")

        # ä½¿ç”¨Ollamaè¿›è¡ŒåµŒå…¥
        status_text.text("æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        embeddings = OllamaEmbeddings(model=embedding_model, base_url=base_url)
        logger.info("Initialized embeddings")
        
        # å‘é‡å­˜å‚¨
        status_text.text("æ­£åœ¨åˆ›å»ºå‘é‡å­˜å‚¨...")
        vector_store = FAISS.from_documents(texts, embeddings)
        logger.info("Created and persisted vector store")
        
        # BM25å­˜å‚¨
        status_text.text("æ­£åœ¨åˆ›å»ºBM25ç´¢å¼•...")
        bm25_retriever = BM25Retriever.from_texts(
            text_contents, 
            bm25_impl=BM25Okapi,
            preprocess_func=lambda text: re.sub(r"\W+", " ", text).lower().split()
        )
        logger.info("Created BM25 retriever")

        # ç»„åˆæ£€ç´¢ç­–ç•¥ (BM25 + å‘é‡æœç´¢)
        status_text.text("æ­£åœ¨é…ç½®æ£€ç´¢ç³»ç»Ÿ...")
        ensemble_retriever = EnsembleRetriever(
            retrievers=[
                bm25_retriever,
                vector_store.as_retriever(search_kwargs={"k": 5})
            ],
            weights=[0.4, 0.6]
        )
        logger.info("Created ensemble retriever")
        # ç”ŸæˆçŸ¥è¯†å›¾è°±
        knowledge_graph = build_knowledge_graph(texts)
        # å°†æ•°æ®å­˜å‚¨åˆ°ä¼šè¯çŠ¶æ€
        st.session_state.retrieval_pipeline = {
            "ensemble": ensemble_retriever,
            "reranker": reranker,
            "texts": text_contents,
            "knowledge_graph": knowledge_graph  # ç”Ÿæˆå¹¶å­˜å‚¨çŸ¥è¯†å›¾è°±
        }

        st.session_state.documents_loaded = True
        st.session_state.processing = False
        
	    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œæ€»æ–‡æ¡£æ•°: {len(texts)}")
        logger.info(f"çŸ¥è¯†å›¾è°±èŠ‚ç‚¹æ•°: {len(knowledge_graph.nodes)}, è¾¹æ•°: {len(knowledge_graph.edges)}")
    
	    # UI æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        st.write(f"ğŸ“œ å¤„ç†çš„æ–‡æ¡£æ•°: {len(texts)}")
        st.write(f"ğŸ”— çŸ¥è¯†å›¾è°±èŠ‚ç‚¹æ•°: {len(knowledge_graph.nodes)}")
        st.write(f"ğŸ”— çŸ¥è¯†å›¾è°±è¾¹æ•°: {len(knowledge_graph.edges)}")


    except Exception as e:
        logger.error(f"Error in document processing pipeline: {str(e)}")
        st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
        raise
    finally:
        progress_bar.empty()
        status_text.empty()
