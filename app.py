import streamlit as st
import requests
import json
from mindtree_retriever import retrieve_documents
from mindtree_doc_handler import process_documents
from sentence_transformers import CrossEncoder
import torch
import os
from dotenv import load_dotenv, find_dotenv
from logger_config import logger


torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)]  # ä¿®å¤torchç±»æœªæ‰¾åˆ°é”™è¯¯
load_dotenv(find_dotenv())  # åŠ è½½.envæ–‡ä»¶å†…å®¹åˆ°åº”ç”¨ç¨‹åºä¸­ï¼Œä½¿å…¶å¯é€šè¿‡os.getenv()è®¿é—®

OLLAMA_BASE_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_API_URL = f"{OLLAMA_BASE_URL}/api/generate"
MODEL = os.getenv("MODEL", "deepseek-r1:7b") # ç¡®ä¿åœ¨ Ollama ä¸­å·²å®‰è£…è¯¥æ¨¡å‹
EMBEDDINGS_MODEL = os.getenv("EMBEDDINGS_MODEL", "nomic-embed-text:latest")
CROSS_ENCODER_MODEL = os.getenv("CROSS_ENCODER_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")


device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device}")

reranker = None  # åˆå§‹åŒ–äº¤å‰ç¼–ç å™¨ï¼ˆé‡æ–°æ’åºå™¨ï¼‰
try:
    reranker = CrossEncoder(CROSS_ENCODER_MODEL, device=device)
except Exception as e:
    st.error(f"åŠ è½½ CrossEncoder æ¨¡å‹å¤±è´¥: {str(e)}")

# Streamlité…ç½®
st.set_page_config(page_title="Mindtree RAG", layout="wide")      

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
    <style>
        /* å…¨å±€æ ·å¼ */
        .stApp { 
            background-color: #f8f9fa;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        /* æ ‡é¢˜æ ·å¼ */
        h1 { 
            color: #2c3e50;
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 1em;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }
        
        /* èŠå¤©æ¶ˆæ¯æ ·å¼ */
        .stChatMessage { 
            border-radius: 15px;
            padding: 15px;
            margin: 15px 0;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .stChatMessage.user { 
            background-color: #e3f2fd;
            border-left: 5px solid #2196f3;
        }
        .stChatMessage.assistant { 
            background-color: #f1f8e9;
            border-left: 5px solid #4caf50;
        }
        
        /* æŒ‰é’®æ ·å¼ */
        .stButton>button { 
            background-color: #1976d2;
            color: white;
            border-radius: 25px;
            padding: 10px 20px;
            transition: all 0.3s ease;
        }
        .stButton>button:hover {
            background-color: #1565c0;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
        }
        
        /* ä¾§è¾¹æ æ ·å¼ */
        .stSidebar {
            background-color: #ffffff;
            box-shadow: 2px 0 5px rgba(0,0,0,0.1);
        }
        
        /* æ–‡æœ¬æ ·å¼ */
        .stMarkdown {
            color: #333;
        }
        
        /* æ»‘å—æ ·å¼ */
        .stSlider {
            padding: 2em 0;
        }
        
        /* å¤é€‰æ¡†æ ·å¼ */
        .stCheckbox {
            padding: 1em 0;
        }
        
        /* æ–‡ä»¶ä¸Šä¼ åŒºåŸŸæ ·å¼ */
        .stUploader {
            border: 2px dashed #1976d2;
            border-radius: 10px;
            padding: 20px;
            margin: 10px 0;
        }
        
        /* è¿›åº¦æ¡æ ·å¼ */
        .stProgress {
            height: 10px;
            border-radius: 5px;
        }
    </style>
""", unsafe_allow_html=True)


# ç®¡ç†ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
    logger.info("Initialized messages session state")
if "retrieval_pipeline" not in st.session_state:
    st.session_state.retrieval_pipeline = None
    logger.info("Initialized retrieval_pipeline session state")
if "rag_enabled" not in st.session_state:
    st.session_state.rag_enabled = True
    logger.info("Initialized rag_enabled session state")
if "documents_loaded" not in st.session_state:
    st.session_state.documents_loaded = False
    logger.info("Initialized documents_loaded session state")
if "enable_hyde" not in st.session_state:
    st.session_state.enable_hyde = True
    logger.info("Initialized enable_hyde session state")
if "enable_reranking" not in st.session_state:
    st.session_state.enable_reranking = True
    logger.info("Initialized enable_reranking session state")
if "enable_graph_rag" not in st.session_state:
    st.session_state.enable_graph_rag = True
    logger.info("Initialized enable_graph_rag session state")
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.3
    logger.info("Initialized temperature session state")
if "max_contexts" not in st.session_state:
    st.session_state.max_contexts = 3
    logger.info("Initialized max_contexts session state")
if "processing" not in st.session_state:
    st.session_state.processing = False
    logger.info("Initialized processing session state")

# ä¾§è¾¹æ 
with st.sidebar:    
    st.header("ğŸ“ æ–‡æ¡£ç®¡ç†")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£ (PDF/DOCX/TXT)",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True
    )
    
    if uploaded_files and not st.session_state.documents_loaded:
        try:
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                logger.info(f"Processing {len(uploaded_files)} uploaded files")
                process_documents(uploaded_files, reranker, EMBEDDINGS_MODEL, OLLAMA_BASE_URL)
                logger.info("Document processing completed successfully")
                st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        except Exception as e:
            logger.error(f"Error processing documents: {str(e)}")
            st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    
    st.markdown("---")
    st.header("ğŸ”† RAGè®¾ç½®")
    
    st.session_state.rag_enabled = st.checkbox("å¯ç”¨RAG", value=True)
    st.session_state.enable_hyde = st.checkbox("å¯ç”¨HyDE", value=True)
    st.session_state.enable_reranking = st.checkbox("å¯ç”¨ç¥ç»é‡æ’åº", value=True)
    st.session_state.enable_graph_rag = st.checkbox("å¯ç”¨GraphRAG", value=True)
    st.session_state.temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.3, 0.05)
    st.session_state.max_contexts = st.slider("æœ€å¤§ä¸Šä¸‹æ–‡æ•°", 1, 5, 3)
    
    if st.button("æ¸…é™¤èŠå¤©å†å²"):
        st.session_state.messages = []
        st.rerun()

    # é¡µè„šï¼ˆä¾§è¾¹æ å³ä¸‹è§’ï¼‰
    st.sidebar.markdown("""
        <div style="position: fixed; bottom: 20px; right: 20px; font-size: 12px; color: #666;">
            <b>ç”± William å¼€å‘</b> &copy; 2025 ä¿ç•™æ‰€æœ‰æƒåˆ©
        </div>
    """, unsafe_allow_html=True)

# èŠå¤©ç•Œé¢
st.title("ğŸŒ³ Mindtree RAG")
st.caption("å…·æœ‰GraphRAGã€æ··åˆæ£€ç´¢ã€ç¥ç»é‡æ’åºå’ŒèŠå¤©å†å²åŠŸèƒ½çš„é«˜çº§RAGç³»ç»Ÿ")

# æ˜¾ç¤ºæ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    try:
        logger.info(f"Received new user prompt: {prompt[:50]}...")
        chat_history = "\n".join([msg["content"] for msg in st.session_state.messages[-5:]])  # æœ€å5æ¡æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ç”Ÿæˆå“åº”
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = ""
            if st.session_state.rag_enabled and st.session_state.retrieval_pipeline:
                try:
                    logger.info("Retrieving documents for context")
                    docs = retrieve_documents(prompt, OLLAMA_API_URL, MODEL, chat_history)
                    context = "\n".join(
                        f"[æ¥æº {i+1}]: {doc.page_content}" 
                        for i, doc in enumerate(docs)
                    )
                    logger.info(f"Retrieved {len(docs)} documents for context")
                except Exception as e:
                    logger.error(f"Error retrieving documents: {str(e)}")
                    st.error(f"æ£€ç´¢é”™è¯¯: {str(e)}")
            
            # ç»“æ„åŒ–æç¤ºè¯
            system_prompt = f"""ä½¿ç”¨èŠå¤©å†å²ä¿æŒä¸Šä¸‹æ–‡ï¼š
                èŠå¤©å†å²ï¼š
                {chat_history}

                é€šè¿‡ä»¥ä¸‹æ­¥éª¤åˆ†æé—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼š
                1. è¯†åˆ«å…³é”®å®ä½“å’Œå…³ç³»
                2. æ£€æŸ¥æ¥æºä¹‹é—´çš„çŸ›ç›¾
                3. ç»¼åˆå¤šä¸ªä¸Šä¸‹æ–‡çš„ä¿¡æ¯
                4. å½¢æˆç»“æ„åŒ–å“åº”

                ä¸Šä¸‹æ–‡ï¼š
                {context}

                é—®é¢˜: {prompt}
                å›ç­”:"""
            
            # æµå¼å“åº”
            logger.info("Generating response using Ollama")
            response = requests.post(
                OLLAMA_API_URL,
                json={
                    "model": MODEL,
                    "prompt": system_prompt,
                    "stream": True,
                    "options": {
                        "temperature": st.session_state.temperature,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„åŠ¨æ€å€¼
                        "num_ctx": 4096
                    }
                },
                stream=True
            )
            try:
                for line in response.iter_lines():
                    if line:
                        data = json.loads(line.decode())
                        token = data.get("response", "")
                        full_response += token
                        response_placeholder.markdown(full_response + "â–Œ")
                        
                        # æ£€æµ‹åˆ°ç»“æŸæ ‡è®°æ—¶åœæ­¢
                        if data.get("done", False):
                            break
                            
                response_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                logger.info("Response generated successfully")
                
            except Exception as e:
                logger.error(f"Error generating response: {str(e)}")
                st.error(f"ç”Ÿæˆé”™è¯¯: {str(e)}")
                st.session_state.messages.append({"role": "assistant", "content": "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ã€‚"})
                
    except Exception as e:
        logger.error(f"Unexpected error in chat processing: {str(e)}")
        st.error("å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·é‡è¯•ã€‚")
